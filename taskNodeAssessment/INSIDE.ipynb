{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of high-influence nodes in boolean models\n",
    "\n",
    "This notebook gathers a study to identify valuable nodes from a predictive model by inverting their activity states (KI and KO) and checking their subsequent changes to model predictions, compared to the WT model. \n",
    "\n",
    "We are testing this approach on four models generated according to the following cell lines: AGS, SW-620, COLO 205 and DU-145. \n",
    "\n",
    "For each cell line, a logical model was manually built and simulated to predict drug combination effects of 18 single drugs. Predictions were tested against our experimental drug screen (Flobak et al., Scientific Data, 2019).\n",
    "\n",
    "In this study, we tested the influence of each single node in the network to predictions of tested drug combinations. For this we applied additional perturbations to drug targets by modifying the activity of single nodes in the network. Each perturbation included a fixed or an inverted node state along with a single drug target perturbation or a combination of two drug targets perturbation. This implies $ 2 (fixed|inverted) * 144 (nodes) * 171 (drug combinations) $ mutations to test per cell line.\n",
    "\n",
    "Once the mutation is set in the model, we compute the stable states using the bioLQM library tool. From the obtain stable states, we checked if the predictive drug synergies are compliant with experimentally observed synergies and whether predictions are altered compared to the WT model. \n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "To simplify the notebook, some functions are defined in `utils.py`  and called in this notebook. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import biolqm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# Set color palette for all plots\n",
    "c_map = sns.cubehelix_palette(dark=0, light=0.8, as_cmap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Data import\n",
    "\n",
    "Import of the combinations of drugs with their targets (nodes) to be knocked out.\n",
    "Import the experimental synergy data for each cell line (HSA synergy). \n",
    "Import the four cell lines models with bioLQM.\n",
    "\n",
    "---\n",
    "**NOTE**\n",
    "\n",
    "`cellLines` should contain a list of tuples where a tuple contains the string name of the cell (identical to column headers in all data) and it's GINML model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import combination of drugs\n",
    "file_comb_drugs = '../data/perturbations.txt'\n",
    "comb_drugs = list(csv.reader(open(file_comb_drugs, 'r'), delimiter='\\t'))\n",
    "\n",
    "# Import drugs and their targets - header is removed\n",
    "file_drug_targets = '../data/drugpanel.txt'\n",
    "drug_targets = list(csv.reader(open(file_drug_targets, 'r'), delimiter='\\t'))[1:]\n",
    "\n",
    "# Combination of drugs and ko of their targets\n",
    "perturbations = utils.get_perturbations(comb_drugs, drug_targets)\n",
    "print('Combinations of drugs loaded in \\'perturbations\\'...')\n",
    "\n",
    "# Free the memory\n",
    "del comb_drugs\n",
    "del drug_targets\n",
    "\n",
    "# Get the number of perturbations and the number of drugs for later analysis\n",
    "nb_perturbations = utils.get_nb_perturbations(perturbations)\n",
    "nb_drugs = utils.get_nb_drugs(perturbations)  \n",
    "\n",
    "# HSA synergy = experimental data observed in each cell line whether a combination of drugs is synergestic or not\n",
    "file_hsa_synergy = '../data/20190205_Synergy_HSA.txt'\n",
    "hsa_synergy = pd.read_csv(file_hsa_synergy, delimiter='\\t')\n",
    "print('Experimental HSA synergy data imported...')\n",
    "\n",
    "# Models to analyse\n",
    "file_ags = \"../data/models/AGS_refined-model.zginml\"\n",
    "file_colo205 = \"../data/models/COLO205_refined-model.zginml\"\n",
    "file_du145 = \"../data/models/DU145_refined-model.zginml\"\n",
    "file_sw620 = \"../data/models/SW620_refined-model.zginml\"\n",
    "\n",
    "# Load models with bioLQM\n",
    "ags_model = biolqm.load(file_ags)\n",
    "colo205_model = biolqm.load(file_colo205)\n",
    "du145_model = biolqm.load(file_du145)\n",
    "sw620_model = biolqm.load(file_sw620)\n",
    "\n",
    "print('Models imported as ginml and loaded with biolqm...')\n",
    "\n",
    "# Create a list of cell lines with the bioLQM model, the name of the cell line\n",
    "cellLines = [('AGS', ags_model), ('COLO205', colo205_model), ('DU145', du145_model), ('SW620', sw620_model)]\n",
    "\n",
    "# Number of double combinations = 153\n",
    "nb_double_comb = nb_perturbations - nb_drugs\n",
    "\n",
    "# Repositories name\n",
    "predictions = \"results/predictions/\"\n",
    "simulations = \"results/simulationStates/\"\n",
    "randomF = \"results/randomForest/\"\n",
    "figures = \"results/figures/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Analysis of the Wild-Type models\n",
    "\n",
    "### 1. Compute stable states with bioLQM (time consuming)\n",
    "Stable states are called fixpoints in bioLQM. If a stable state is found, the growth of the cell is calculated:\n",
    "\n",
    "$ Growth = Prosurvival - Antisurvival$. \n",
    "\n",
    "This is necessary to predict synergystic effects of a drug combination. \n",
    "\n",
    "The results (mutation with inverted/fixed node, drugs - growth - nb_fixpoints) are stored in `results/simulationStates/`*`model`*`_WT_growth.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Note** \n",
    "\n",
    "If multiple stable states are found, the mean of the Prosurvival & the mean of the Antisurvival are calculated to provide a unique Growth value.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('This may take some time, please be patient...')\n",
    "for i in range(len(cellLines)):\n",
    "    file_fxpts = simulations+ str((cellLines[i])[0])+'_WT_growth.txt'\n",
    "    fixpoints = open(file_fxpts, \"w+\")\n",
    "    utils.get_stable_states_wt(fixpoints, cellLines[i][1], perturbations)\n",
    "    print('Computed stable states for ' + (cellLines[i])[0])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis of stable states with experimental HSA synergy data \n",
    "Classify the prediction into different categories by defining whether the prediction is a __true positive__ (TP), __true negative__ (TN), __false positive__ (FP) or __false negative__ (FN) result based on the experimental HSA synergy data.\n",
    "\n",
    "\n",
    "These metrics will be used for comparison with mutated nodes states performed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute predictions: synergy or not? Compare with HSA synergy data\n",
    "print('Computation of predictions...')\n",
    "\n",
    "phenotype = 'WT'\n",
    "\n",
    "for i in range(len(cellLines)): \n",
    "    # Read fixpoints to compute predictions\n",
    "    file_fxpts = simulations+(cellLines[i])[0]+'_WT_growth.txt'\n",
    "    file_pred = predictions+ str((cellLines[i])[0])+'_WT.txt'\n",
    "    synergy = hsa_synergy[['Combination', (cellLines[i])[0]]]\n",
    "    # Write down the predictions\n",
    "    utils.get_predictions(file_fxpts, file_pred, synergy, nb_perturbations, nb_drugs, phenotype, (cellLines[i])[0])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Analysis of fixed nodes activities and inverted nodes activities models\n",
    "\n",
    "### 1. Compute stable states for fixed nodes and inverted nodes activities (time consuming)\n",
    "\n",
    "Import the activities of each node in specific cell lines (__node_activities__) and invert their activity (__inverted_node_activities__) in order to apply a mutation to the models.\n",
    "\n",
    "For the models with an inversion of node:\n",
    "* If a node is active in a cell line model, we do a knock-out\n",
    "* If a node is inactive in a cell line model, we do an knock-in (ectopic mutation)\n",
    "\n",
    "It is the other way around for the study of fixed node activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import file with node activities for each model\n",
    "node_activity_file = '../data/20180828_Ginsim_node_activity.txt'\n",
    "fixed_nodes_activities = pd.read_csv(node_activity_file, sep='\\t', index_col=0)\n",
    "\n",
    "# Set empty dataframe for inverted node activities\n",
    "inverted_nodes_activities = pd.DataFrame(columns=list(fixed_nodes_activities.columns.values), index= list(fixed_nodes_activities.index.values))\n",
    "\n",
    "# Invert activity of nodes: 1 becomes 0 and 0 becomes 1\n",
    "for node in fixed_nodes_activities.iterrows():\n",
    "    inverted_nodes_activities.loc[node[0]] = 1 - node[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the study for the WT models, we define a function to automatically:\n",
    "\n",
    "* Compute the stable states for each mutation\n",
    "    * Fix the node activities as measured in GINsim (apply a mutation) and compute the stable states for each perturbation\n",
    "    * Invert the node activities measured and do the same process as in step one.\n",
    "* Get Prosurvival node and Antisurvival node values\n",
    "* Calculate Growth: $ Growth = Prosurvival - Antisurvival$\n",
    "* If multiple stable states are found, get the mean of the Prosurvival & the mean of the Antisurvival to calculate a unique Growth value.\n",
    "* Store the mutation (mutated node - targets KO - drugs) - growth - nb_fixpoints - stablestate in the results folder, for example: `results/simulationStates/AGS_fixed_growth.txt`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Note**\n",
    "\n",
    "This is a time consuming computation. The results are already computed and available (look for `results/simulationStates/`*`model`*`_fixed_growth.txt` or `results/simulationStates/`*`model`*`_inverted_growth.txt`). If you don't want to run the analysis, you can directly go to the section `Analysis of stable states with experimental HSA synergy data`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('This will take some time, please be patient...')\n",
    "for i in range(len(cellLines)):\n",
    "    print('Computing fixed node activities mutations')\n",
    "    file_fixed_fxpts = simulations+(cellLines[i])[0]+'_fixed_growth.txt'\n",
    "    utils.get_stable_states_mut((cellLines[i])[1], file_fixed_fxpts, fixed_nodes_activities[[(cellLines[i])[0]]], perturbations)\n",
    "\n",
    "    print('Computing inverted node activities mutations')\n",
    "    file_inverted_fxpts = simulations+(cellLines[i])[0]+'_inverted_growth.txt'\n",
    "    utils.get_stable_states_mut((cellLines[i])[1], file_inverted_fxpts, inverted_nodes_activities[[(cellLines[i])[0]]], perturbations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Analysis of stable states with experimental HSA synergy data\n",
    "Define a function that goes through the resulting stable states of each cell line to:\n",
    "* Split the data into chunks of inverted nodes stable states results: 1 inverted node has 171 combination of perturbations computed.\n",
    "* For each of the inverted model, find the drug synergies by comparing single drugs *(AK, BI)* with double drugs *(AK-BI)* analysis\n",
    "    * Create a triplet of information on both the single drugs and the double drugs KO and go through this list\n",
    "    * Get the according HSA synergy experimental data\n",
    "    * If fixpoints are missing: define what is missing:\n",
    "        * __none__: when both single drugs and combination don't find a fixpoint\n",
    "        * __double__: When both single drugs don't find a fixpoint but the combination does\n",
    "        * __single__: when the combination doesn't find a fixpoint but both single drugs do\n",
    "        * __either__: when either one of the single drug or the combination of drugs don't find a fixpoint\n",
    "    * If fixpoints are found: \n",
    "        * Define whether the prediction is a __true positive__ (TF), __true negative__ (TN), __false positive__ (FP) or __false negative__ (FN) result based on the HSA synergy data.\n",
    "        \n",
    "The predictions are stored in a separate file for each cell line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Computation of predictions... this may take some time')\n",
    "#4min per cell line\n",
    "\n",
    "phenotype = 'mutant'\n",
    "\n",
    "for i in range(len(cellLines)):\n",
    "    print(cellLines[i][0])\n",
    "    # Read both fixed and inverted fixpoints \n",
    "    file_fixed_fxpts = simulations+(cellLines[i])[0]+'_fixed_growth.txt'\n",
    "    file_inv_fxpts = simulations+(cellLines[i])[0]+'_inverted_growth.txt'\n",
    "    \n",
    "    # Get the synergy column of the cell line of interest\n",
    "    synergy = hsa_synergy[['Combination', (cellLines[i])[0]]]\n",
    "    \n",
    "    # Output files: store predictions\n",
    "    file_inv_pred = predictions+(cellLines[i])[0]+'_inverted.txt'\n",
    "    file_fixed_pred = predictions+(cellLines[i])[0]+'_fixed.txt'\n",
    "    \n",
    "    # Compute predictions of fixed and inverted models\n",
    "    utils.get_predictions(file_inv_fxpts, file_inv_pred, synergy, nb_perturbations, nb_drugs, phenotype, (cellLines[i])[0])\n",
    "    utils.get_predictions(file_fixed_fxpts, file_fixed_pred, synergy, nb_perturbations, nb_drugs, phenotype, (cellLines[i])[0])\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Comparison of results between WT and mutants\n",
    "We now classify nodes according to their importance on the different models. \n",
    "The importance or influential character of a node is assessed on whether the fixation (or the inversion) of it's activity has changed the predictions compared to the WT analysis. \n",
    "\n",
    "First, we classify the observations into four categories, from the WT to the mutant:\n",
    "- gain of TP\n",
    "- gain of TN\n",
    "- loss of TP\n",
    "- loss of TN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(cellLines)):\n",
    "    # Load prediction files to read \n",
    "    file_inv_pred = predictions+(cellLines[i])[0]+'_inverted.txt'\n",
    "    file_fix_pred = predictions+(cellLines[i])[0]+'_fixed.txt'\n",
    "    file_WT_pred = predictions+(cellLines[i])[0]+'_WT.txt'\n",
    "    \n",
    "    # Write classification of gain and loss - return classification file name\n",
    "    file_classification_fix = utils.get_classification_gain_loss(file_fix_pred, file_WT_pred, nb_double_comb)\n",
    "    file_classification_inv = utils.get_classification_gain_loss(file_inv_pred, file_WT_pred, nb_double_comb)\n",
    "    \n",
    "    # Create heatmaps\n",
    "    utils.get_classification_heatmap(file_classification_fix, c_map)\n",
    "    utils.get_classification_heatmap(file_classification_inv, c_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a list of all the nodes\n",
    "nodes_list = []\n",
    "with open('results/AGS_fixed_classification_gainloss.txt') as csvFile:\n",
    "    reader = csv.reader(csvFile, delimiter = '\\t')\n",
    "    next(reader)\n",
    "    for line in reader:\n",
    "        nodes_list.append(line[0][:-2])\n",
    "\n",
    "# Create an empty dataframe with row names corresponding to the nodes of the model\n",
    "influentialNodes = pd.DataFrame(index = nodes_list)\n",
    "\n",
    "def set_influential_node(model):\n",
    "    list_influence = []\n",
    "\n",
    "    mod_f = open('results/' + model + '_fixed_classification_gainloss.txt', 'r')\n",
    "    mod_f = csv.reader(mod_f, delimiter='\\t')\n",
    "    mod_f = list(mod_f)\n",
    "\n",
    "    mod_i = open('results/' + model + '_inverted_classification_gainloss.txt', 'r')\n",
    "    mod_i = csv.reader(mod_i, delimiter='\\t')\n",
    "    mod_i = list(mod_i)\n",
    "\n",
    "    for i in range(1, len(mod_f), 1):\n",
    "        if (int(mod_f[i][1]) + int(mod_f[i][2]) + int(mod_f[i][3]) + int(mod_f[i][4]) + int(mod_f[i][5]) + int(\n",
    "                mod_i[i][1]) + int(mod_i[i][2]) + int(mod_i[i][3]) + int(mod_i[i][4]) + int(mod_i[i][5])) == 0:\n",
    "            list_influence.append('0')\n",
    "        else:\n",
    "            list_influence.append('1')\n",
    "    return list_influence\n",
    "    \n",
    "def rank_per_cell(model):\n",
    "    list_ranked = []\n",
    "    mod_f = open('results/'+ model +'_fixed_classification_gainloss.txt', 'r')\n",
    "    mod_f = csv.reader(mod_f, delimiter='\\t')\n",
    "    mod_f = list(mod_f)\n",
    "    \n",
    "    mod_i = open('results/'+ model +'_inverted_classification_gainloss.txt', 'r')\n",
    "    mod_i = csv.reader(mod_i, delimiter='\\t')\n",
    "    mod_i = list(mod_i)\n",
    "    \n",
    "    for i in range(1,len(mod_f), 1):\n",
    "        sum_rank = mod_f[i][1:5] + mod_i[i][1:5]\n",
    "        sum_rank  = list(map(int, sum_rank))\n",
    "        list_ranked.append(sum(sum_rank))\n",
    "    return list_ranked\n",
    "    \n",
    "influentialNodes['AGS'] = set_influential_node('AGS')\n",
    "influentialNodes['AGS_rank'] = rank_per_cell('AGS')\n",
    "influentialNodes['COLO205'] = set_influential_node('COLO205')\n",
    "influentialNodes['COLO205_rank'] = rank_per_cell('COLO205')\n",
    "influentialNodes['SW620'] = set_influential_node('SW620')\n",
    "influentialNodes['SW620_rank'] = rank_per_cell('SW620')\n",
    "influentialNodes['DU145'] = set_influential_node('DU145')\n",
    "influentialNodes['DU145_rank'] = rank_per_cell('DU145')\n",
    "\n",
    "list_all = []\n",
    "list_any = []\n",
    "\n",
    "for index, row in influentialNodes.iterrows():\n",
    "    if (int(row[0]) + int(row[2]) + int(row[4]) + int(row[6])) == 0:\n",
    "        list_all.append('0')\n",
    "        list_any.append('0')\n",
    "    else:\n",
    "        if (int(row[0]) + int(row[2]) + int(row[4]) + int(row[6])) == 4:\n",
    "            list_all.append('1')\n",
    "            list_any.append('1')\n",
    "        else:\n",
    "            list_all.append('0')\n",
    "            list_any.append('1')\n",
    "influentialNodes['allCells'] = list_all\n",
    "influentialNodes['anyCells'] = list_any\n",
    "\n",
    "influentialNodes.to_csv('results/Classification_influential_nodes.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification of influential nodes based on Random Forest\n",
    "\n",
    "A random forest analysis has been performed taking into account biological and network features to assess whether any feature would allow to classify a node as influential or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_rf_results = randomF + 'importance.tsv'\n",
    "rf_results = pd.read_csv(file_rf_results, sep='\\t', header = 1, index_col=0)\n",
    "\n",
    "file_error = randomF + 'batch'\n",
    "error = pd.read_csv(file_error, sep = '\\t', header= 1, index_col=0)\n",
    "\n",
    "def keep_cols(DataFrame, keep_these):\n",
    "    \"\"\"Keep only the columns [keep_these] in a DataFrame, delete all other columns.\"\"\"\n",
    "    drop_these = list(set(list(DataFrame)) - set(keep_these))\n",
    "    return DataFrame.drop(drop_these, axis = 1)\n",
    "\n",
    "rf_results.fillna(value=0)\n",
    "\n",
    "l_imp = []\n",
    "for val in error.iterrows():\n",
    "    if(val[1]['Error'] < 0.4):\n",
    "        l_imp.append(val[0])\n",
    "    else:\n",
    "        print(val)\n",
    "\n",
    "df = keep_cols(rf_results, l_imp)\n",
    "\n",
    "def filterDataInterest(index, filter_name):\n",
    "    column_renaming = [\"AGS\", \"COLO205\", \"SW620\", \"DU145\", \"allCells\", \"anyCells\"]\n",
    "    data = df[df.filter(like=filter_name).columns]\n",
    "    if isinstance(index, list): \n",
    "        data = data.iloc[index[0]:index[1], :]\n",
    "    else:\n",
    "        data = data.iloc[index:, :]\n",
    "    data.columns = column_renaming\n",
    "    return data\n",
    "\n",
    "'''Balance data - numeric features'''\n",
    "filter_col = [0,13]\n",
    "df_num_balanced = filterDataInterest(filter_col, \"balance_data-numeric_features.txt\")\n",
    "heatmap_n = sns.clustermap(df_num_balanced, figsize=(7,5), col_cluster=False, cmap = c_map)\n",
    "plt.setp(heatmap_n.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "heatmap_n.savefig(figures+'cluster_RF_balanced_numeric_feature.svg')\n",
    "\n",
    "'''Non-balanced data - numeric features'''\n",
    "df_num_nonbalanced = filterDataInterest(filter_col, \"non_balanced_data-numeric_features.txt\")\n",
    "heatmap_nb = sns.clustermap(df_num_nonbalanced, figsize=(7,5), col_cluster=False, cmap = c_map)\n",
    "plt.setp(heatmap_nb.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "heatmap_nb.savefig(figures+'cluster_RF_non-balanced_numeric_feature.svg')\n",
    "\n",
    "'''Balanced data - biological features'''\n",
    "filter_col = 14\n",
    "df_binary_balanced = filterDataInterest(filter_col, \"balance_data-binary_all.txt\")\n",
    "heatmap_b = sns.clustermap(df_binary_balanced, figsize=(20,30), col_cluster = False, cmap = c_map, yticklabels=True)\n",
    "heatmap_b.savefig(figures+'cluster_RF_balanced_bio_feature.svg')\n",
    "\n",
    "''' Non-balanced data - biological features'''\n",
    "df_binary_balanced = filterDataInterest(filter_col, \"non_balanced_data-binary_all.txt\")\n",
    "heatmap_bnb = sns.clustermap(df_binary_balanced, figsize=(20,30), col_cluster = False, cmap = c_map, yticklabels=True)\n",
    "heatmap_bnb.savefig(figures+'cluster_RF_non_balanced_bio_feature.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmaps above, there are only network features that seem to distinguish the nodes and classify them into two categories: PCI, Closeness centrality and Betweenness centrality. It doesn't look like any biological features stands out.\n",
    "\n",
    "#### Network features combination \n",
    "\n",
    "We would like to see now if a combination of these 3 features allows to completely sort the nodes.\n",
    "The following graphs generate 3D scatter plots where nodes are projected according to the combination of PCI, Betweenness centrality and Closeness centrality, in the different cell lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter_3d(x, y, z, points, c_map, title, outputfile): \n",
    "    plot = plt.figure(figsize=(10,8)).gca(projection='3d')\n",
    "    ax = plot.scatter(x, y, z, c = points, cmap =  c_map)\n",
    "    plot.set_xlabel('PCI')\n",
    "    plot.set_ylabel('Betweenness centrality')\n",
    "    plot.set_zlabel('Closeness centrality')\n",
    "    plot.set_title(title)\n",
    "    clb = plt.colorbar(ax)\n",
    "    clb.ax.set_title('Important node')\n",
    "    plt.savefig(outputfile,  dpi = 300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "file_node_features = '../data/features/node_features.txt'\n",
    "file_influential_node = 'results/Classification_influential_nodes.txt'\n",
    "\n",
    "df_node_features = pd.read_csv(file_node_features, sep='\\t', header = 0, index_col=0)\n",
    "df_influential_nodes = pd.read_csv(file_influential_node, sep = '\\t', header = 0, index_col = 0)\n",
    "df = pd.concat([df_node_features, df_influential_nodes], axis=1, sort=False)\n",
    "\n",
    "for i in range(len(cellLines)): \n",
    "    plot_scatter_3d(df[['PCI']], df[['BetweennessCentrality']], df[['ClosenessCentrality']], df['AGS_rank'], c_map, \n",
    "                'Scatter plot of nodes ranked on their importance in '+(cellLines[i])[0], \n",
    "                figures+'scatter_3dplot_features_'+(cellLines[i])[0].lower()+'.svg')\n",
    "\n",
    "# Plot for all cell lines\n",
    "plot_scatter_3d(df[['PCI']], df[['BetweennessCentrality']], df[['ClosenessCentrality']], df['allCells'], c_map,\n",
    "                'Scatter plot of nodes classified as important (1) or not (0) in all cells', \n",
    "                figures+'scatter_3dplot_features_allCells.svg')\n",
    "\n",
    "# Plot for any of the cell lines\n",
    "plot_scatter_3d(df[['PCI']], df[['BetweennessCentrality']], df[['ClosenessCentrality']], df['anyCells'], c_map, \n",
    "                'Scatter plot of nodes classified as important (1) or not (0) in any cells', \n",
    "                figures+'scatter_3dplot_features_anyCells.svg')\n",
    "\n",
    "#Plot for the different cell lines\n",
    "for i in range(len(cellLines)): \n",
    "    print((cellLines[i])[0].lower())\n",
    "    plot_scatter_3d(df[['PCI']], df[['BetweennessCentrality']], df[['ClosenessCentrality']], df[(cellLines[i])[0]+'_rank'], c_map, \n",
    "                'Scatter plot of nodes ranked on their importance in '+(cellLines[i])[0], \n",
    "                figures+'scatter_3dplot_features_'+(cellLines[i])[0].lower()+'.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
